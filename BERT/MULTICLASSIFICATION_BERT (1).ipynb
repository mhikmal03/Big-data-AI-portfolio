{"cells":[{"cell_type":"markdown","metadata":{"id":"WnQQyOmXuJJ-"},"source":["# **TEXT CLASSIFICATION USING BERT**"]},{"cell_type":"markdown","source":["**APA ITU BERT?**\n","\n","BERT (Bidirectional Encoder Representations from Transformers) adalah model pre-trained yang dirancang untuk memahami konteks kata dengan membaca teks secara dua arah (kiri dan kanan). BERT unggul dalam tugas seperti text classification karena:\n","\n","**Transfer Learning**: Sudah dilatih pada dataset besar (Wikipedia, BookCorpus).\n","\n","**Fine-Tuning**: Dapat disesuaikan untuk tugas spesifik seperti klasifikasi teks."],"metadata":{"id":"GWJqAEUfH8Cw"}},{"cell_type":"markdown","source":[" **1. EXPLORATORY DATA ANALYSIS AND** **PREPROCESSING**"],"metadata":{"id":"LJYrcxAuIYcU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BiuEaPZqQLG"},"outputs":[],"source":["! pip install torch torchvision"]},{"cell_type":"code","source":["! pip install tqdm"],"metadata":{"id":"hLBmQvBb6CYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import pandas as pd\n","from tqdm.notebook import tqdm"],"metadata":{"id":"ouI6VdqC6DCA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","#Load dataset\n","df = pd.read_csv ('/content/data_news_practice.csv')"],"metadata":{"id":"JYq7DlOD6dkI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head(10)"],"metadata":{"id":"k19bRaD06-jY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop satu kolom\n","df = df.drop(columns=['news_headline'])"],"metadata":{"id":"DMQv0cdH9hMH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Menghapus kolom news_headline, karena kita fokus ke news_article"],"metadata":{"id":"B7PYRHbJIwIk"}},{"cell_type":"code","source":["df.head(10)"],"metadata":{"id":"m8-NygEi9lyo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.news_category.value_counts()"],"metadata":{"id":"Gr85nOXr7Da3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Menampilkan jumlah data dari setiap label"],"metadata":{"id":"5n4SfbiKLjTU"}},{"cell_type":"code","source":["possible_labels = df.news_category.unique()"],"metadata":{"id":"WfecupHf7UC9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_dict = {}\n","for index, possible_label in enumerate(possible_labels):\n","    label_dict[possible_label] = index"],"metadata":{"id":"xbLjdjnq_axl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Merubah news_category menjadi label_dict"],"metadata":{"id":"0_E6uq3kI655"}},{"cell_type":"code","source":["label_dict"],"metadata":{"id":"EWZ9t-eK_fC3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Memberi label untuk setiap category, pada case ini kategori technology itu bernilai 0, sports 1, world 2, dst"],"metadata":{"id":"Flba2m8vJBWe"}},{"cell_type":"code","source":["df['label'] = df.news_category.replace(label_dict)"],"metadata":{"id":"epOM-1IK_iJS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["merubah label_dict menjadi label"],"metadata":{"id":"zIBYKWuQJLNT"}},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"ZjOj0FIU_nAA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**2. TRAINING AND VALIDATION SPLIT**"],"metadata":{"id":"DIpf7wSJJOpD"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"metadata":{"id":"_IaqGmfk_pPO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["train_test_split dari sklearn.model_selection digunakan untuk membagi dataset menjadi dua subset, yaitu subset pelatihan (training set) dan subset pengujian (test set). Proses ini umumnya dilakukan sebagai langkah pertama dalam mengembangkan model machine learning."],"metadata":{"id":"Av2uHc1BJYZK"}},{"cell_type":"code","source":["x_train, x_val, y_train, y_val =  train_test_split(df.index.values,\n","                                                   df.label.values,\n","                                                   test_size=0.15,\n","                                                   random_state=17,\n","                                                   stratify=df.label.values\n",")"],"metadata":{"id":"h3Xb9gQL_seG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["test size 0.15/15% adalah seberapa banyak data ini disiapkan untuk pelatihan\n","\n","random state adalah parameter yang sering digunakan dalam fungsi-fungsi pembagian data atau model di library Python seperti scikit-learn. Fungsinya adalah mengontrol atau menentukan seed untuk pengacakan (random seed) sehingga proses acak menghasilkan output yang konsisten.\n"],"metadata":{"id":"VADSXLoXLtLH"}},{"cell_type":"code","source":["df['df_type'] = ['not_set']*df.shape[0]"],"metadata":{"id":"j1BAdjDc_ypJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Membuat kolom baru df_type yang akan berisikan not_set untuk semua sample"],"metadata":{"id":"89XZKU13MN8k"}},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"gv3Qb0sq_0o5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.loc[x_train, 'data_type'] = 'train'\n","df.loc[x_val, 'data_type'] = 'val'"],"metadata":{"id":"xPYf5rVY_3MS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.groupby(['news_category', 'label', 'data_type']).count()"],"metadata":{"id":"5fMNUvMH_50i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dari data diatas dapat dilihat bahwa setiap category sudah membagi data train dan data validationnya masing masing"],"metadata":{"id":"VhufPHHZJi-e"}},{"cell_type":"markdown","source":[],"metadata":{"id":"jdOgLCUyMhI3"}},{"cell_type":"markdown","source":["**3. LOADING TOKENIZER AND ENOCODING DATA**"],"metadata":{"id":"XcAXSerWJqmI"}},{"cell_type":"markdown","source":["Tokenizer mengambil teks mentah sebagai input dan membaginya menjadi Token, angka\n","numerik yang mewakili kata tertentu. Tokenizer mengubah teks menjadi data numerik."],"metadata":{"id":"EftfnVX9MgXJ"}},{"cell_type":"code","source":["! pip install transformers"],"metadata":{"id":"OcFemKMy_-sz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","from torch.utils.data import TensorDataset"],"metadata":{"id":"L2ShwRBIACPo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TOKENIZER**"],"metadata":{"id":"pjHWY03uJ1ZL"}},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n","                                          do_lower_case=True)"],"metadata":{"id":"uCserzEmAE_6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tokenizer diambil dari BERT yang sudah di latih sebelumnya\n","\n","bert-base-uncased' berarti harus menggunakan semua data huruf kecil\n","\n","do_lower_case digunakan untuk mengubah semua data menjadi huruf kecil."],"metadata":{"id":"J2fyf_DFMo-b"}},{"cell_type":"markdown","source":["**ENCODING**"],"metadata":{"id":"uZDzzFZ6J5cT"}},{"cell_type":"markdown","source":["Selanjutnya pada tahap ini kita akan mengkonversikan semua data ke dalam bentuk yang\n","disandikan."],"metadata":{"id":"M4u2FR0PM1um"}},{"cell_type":"code","source":["# Encoding the Training data\n","encoded_data_train = tokenizer.batch_encode_plus(\n","    df[df.data_type=='train'].news_article.values,\n","    add_special_tokens=True,\n","    return_attention_mask=True,\n","    pad_to_max_length=True,\n","    max_length=512,\n","    return_tensors='pt'\n",")\n","\n","# Encoding the Validation data\n","encoded_data_val = tokenizer.batch_encode_plus(\n","    df[df.data_type=='val'].news_article.values,\n","    add_special_tokens=True,\n","    return_attention_mask=True,\n","    pad_to_max_length=True,\n","    max_length=512,\n","    return_tensors='pt'\n",")\n","\n","# Spliting the data for the BERT training\n","input_ids_train = encoded_data_train['input_ids']\n","attention_masks_train = encoded_data_train['attention_mask']\n","labels_train = torch.tensor(df[df.data_type=='train'].label.values)\n","\n","input_ids_val = encoded_data_val['input_ids']\n","attention_masks_val = encoded_data_val['attention_mask']\n","labels_val = torch.tensor(df[df.data_type=='val'].label.values)"],"metadata":{"id":"6udLeQS3AHY1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["max_length untuk BERT LM itu biasanya antara 128/256/512 (bisa lebih jika model yang digunakan berbeda), max_length itu untuk menghitung jumlah kata pada setiap kolom data.\n","\n","pada case ini, max length yang digunakan 512 karena data yang di analisis itu adalah artikel dari berita"],"metadata":{"id":"8a5Y-aorM7a_"}},{"cell_type":"markdown","source":["**4. CHANGE INPUT INTO BERT ALGORITHM**"],"metadata":{"id":"U_bhPUGmJ_Vl"}},{"cell_type":"code","source":["# Creating two different dataset\n","dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n","dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"],"metadata":{"id":"l0ihpe7nAb2a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(dataset_train)"],"metadata":{"id":"alhzWnf_AeeQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1364 data akan menjadi training"],"metadata":{"id":"uoMzCW9zKGrp"}},{"cell_type":"code","source":["len(dataset_val)"],"metadata":{"id":"_Sc5FZt9Agqx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["241 data akan di validasi"],"metadata":{"id":"6rW79tQ2KIim"}},{"cell_type":"markdown","source":["**5. SETTING UP BERT PRE TRAINED MODEL**"],"metadata":{"id":"olsDlXUnKSS5"}},{"cell_type":"code","source":["from transformers import BertForSequenceClassification\n","\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                      num_labels=len(label_dict),\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)"],"metadata":{"id":"w3kvmzIRAjvK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Memuat sebuah model kecerdasan buatan yang bernama BERT, yang sudah dilatih untuk mengenali jenis-jenis kategori teks tertentu"],"metadata":{"id":"zsaGX-msKYUl"}},{"cell_type":"markdown","source":["**6. CREATING DATA LOADER**\n","\n","Dataloader menggabungkan kumpulan data dan sampler, dan menyediakan iterator tunggal\n","atau multiproses di atas kumpulan data. Dataloader yang akan digunakan ada 2, yaitu\n","dataloader untuk data train dan validation"],"metadata":{"id":"YuiaUq-LN_zO"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"],"metadata":{"id":"hgV54WpwAmfc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 16\n","\n","# We Need two different dataloder\n","dataloader_train = DataLoader(dataset_train,\n","                              sampler=RandomSampler(dataset_train),\n","                              batch_size=batch_size)\n","\n","dataloader_validation = DataLoader(dataset_val,\n","                              sampler=RandomSampler(dataset_val),\n","                              batch_size=batch_size)"],"metadata":{"id":"yQA6WbRBAsKP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**7. SETTING UP OPTIMISER AND SCHEDULER**"],"metadata":{"id":"DuFfjUnqKji5"}},{"cell_type":"code","source":["from transformers import AdamW, get_linear_schedule_with_warmup"],"metadata":{"id":"89yyTtrWAt4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = AdamW(model.parameters(),\n","                  lr=1e-5,\n","                  eps=1e-8)"],"metadata":{"id":"GR4z6BOxAwg8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Algoritma AdamW: menghitung peluruhan bobot sebelum menerapkan langkah gradien."],"metadata":{"id":"JO4tvnrqOLE5"}},{"cell_type":"code","source":["epochs = 3\n","\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps=0,\n","                                            num_training_steps=len(dataloader_train)*epochs)"],"metadata":{"id":"zZKJlY2LAyps"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Epoch menunjukkan berapa kali kita ingin melatih data kita, disini kita akan melakukan pelatihan sebanyak 3x"],"metadata":{"id":"iELGiAwHKrCD"}},{"cell_type":"markdown","source":["**7. DEFINING OUR PERFORMANCE METRICS**"],"metadata":{"id":"tf5d-jEqKveL"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"-6h9N6MqA39g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import f1_score"],"metadata":{"id":"6_v5QcF_A4zu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def f1_score_func(preds, labels):\n","\n","    # Setting up the preds to axis=1\n","    # Flatting it to a single iterable list of array\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","\n","    # Flattening the labels\n","    labels_flat = labels.flatten()\n","\n","    # Returning the f1_score as define by sklearn\n","    return f1_score(labels_flat, preds_flat, average='weighted')"],"metadata":{"id":"1p5Hi7vaA9_l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def accuracy_per_class(preds, labels):\n","    label_dict_inverse = {v: k for k, v in label_dict.items()}\n","\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    # Iterating over all the unique labels\n","    # label_flat are the --> True labels\n","    for label in np.unique(labels_flat):\n","        # Taking out all the pred_flat where the True alable is the lable we care about.\n","        # e.g. for the label Happy -- we Takes all Prediction for true happy flag\n","        y_preds = preds_flat[labels_flat==label]\n","        y_true = labels_flat[labels_flat==label]\n","        print(f'Class: {label_dict_inverse[label]}')\n","        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"],"metadata":{"id":"_TjKlbDDBAB6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**8. IMPORT TRAINING LOOP TO CONTROL PYTORCH FINETUNING OF BERT USING CPU OR GPU ACCELERATION**"],"metadata":{"id":"oslWEVuOK1p8"}},{"cell_type":"markdown","source":["Pada tahap ini membuat loop pelatihan untuk mengontrol finetuning PyTorch BERT\n","menggunakan akselerasi CPU atau GPU."],"metadata":{"id":"cbQ8qjm6OU6g"}},{"cell_type":"code","source":["import random\n","\n","seed_val = 17\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)"],"metadata":{"id":"QwJpEl7HBCSY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","print(device)"],"metadata":{"id":"itZcVna8BFWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(dataloader_val):\n","\n","    model.eval()\n","\n","    loss_val_total = 0\n","    predictions, true_vals = [], []\n","\n","    for batch in tqdm(dataloader_val):\n","\n","        batch = tuple(b.to(device) for b in batch)\n","\n","        inputs = {'input_ids':      batch[0],\n","                  'attention_mask': batch[1],\n","                  'labels':         batch[2],\n","                 }\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        loss = outputs[0]\n","        logits = outputs[1]\n","        loss_val_total += loss.item()\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = inputs['labels'].cpu().numpy()\n","        predictions.append(logits)\n","        true_vals.append(label_ids)\n","\n","    loss_val_avg = loss_val_total/len(dataloader_val)\n","\n","    predictions = np.concatenate(predictions, axis=0)\n","    true_vals = np.concatenate(true_vals, axis=0)\n","\n","    return loss_val_avg, predictions, true_vals"],"metadata":{"id":"x4tOLRnOBHp7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Setiap epoch akan melakukan training dan validation dan akan menghasilkan training loss,\n","validation loss, dan F1-Score.\n","\n","• Training loss adalah kesalahan pada training set data\n","\n","• Validation loss adalah kesalahan setelah menjalankan set validasi data melalui\n","jaringan yang dilatih.\n","\n","• F1-Score adalah rata rata dari precision dan recall"],"metadata":{"id":"qgdgWzOfObpS"}},{"cell_type":"code","source":["for epoch in tqdm(range(1, epochs+1)):\n","\n","    model.train()\n","\n","    loss_train_total = 0\n","\n","    # Setting up the Progress bar to Moniter the progress of training\n","    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n","    for batch in progress_bar:\n","\n","        model.zero_grad() # As we not working with thew RNN's\n","\n","        # As our dataloader has '3' iteams so batches will be the Tuple of '3'\n","        batch = tuple(b.to(device) for b in batch)\n","\n","        # INPUTS\n","        # Pulling out the inputs in the form of dictionary\n","        inputs = {'input_ids':      batch[0],\n","                  'attention_mask': batch[1],\n","                  'labels':         batch[2],\n","                 }\n","\n","        # OUTPUTS\n","        outputs = model(**inputs) # '**' Unpacking the dictionary stright into the input\n","\n","        loss = outputs[0]\n","        loss_train_total += loss.item()\n","        loss.backward()           # backpropagation\n","\n","        # Gradient Clipping -- Taking the Grad. & gives it a NORM value ~ 1\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","        scheduler.step()\n","\n","        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","\n","\n","    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')\n","\n","    tqdm.write(f'\\nEpoch {epoch}')\n","\n","    loss_train_avg = loss_train_total/len(dataloader_train)\n","    tqdm.write(f'Training loss: {loss_train_avg}')\n","\n","    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n","    val_f1 = f1_score_func(predictions, true_vals)\n","    tqdm.write(f'Validation loss: {val_loss}')\n","    tqdm.write(f'F1 Score (Weighted): {val_f1}')"],"metadata":{"id":"vfU2lRPZBNww"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["• Jika training loss dan validation loss terus turun, naikan epoch dua kali lipatnya. Katakanlah epoch selanjutnya 6.\n","\n","• Jika pada epoch 6 training loss dan validation loss terlihat stagnan (tidak mengalami\n","penurunan), coba naikan kompleksitas mode (menambah hidden layer).\n","\n","• Jika pada epoch 6, training loss dan validation loss berlawanan (validation loss\n","meningkat dan training loss menurun, atau sebaliknya), maka itu mengindikasikan\n","model kita overfitting. Overfitting adalah suatu keadaan dimana data yang digunakan\n","untuk pelatihan itu adalah yang \"terbaik\". Sehingga apabila dilakukan tes dengan\n","menggunakan data yang berbeda dapat mengurangi akurasi (hasil yang dibuat tidak\n","sesuai yang diharapkan).\n","\n","• Jika pada epoch 6, validation loss jauh lebih kecil dibandingkan training loss ini\n","mengindikasikan underfitting. Underfitting adalah keadaan dimana model pelatihan\n","data yang dibuat tidak mewakilkan keseluruhan data yang akan digunakan nantinya.\n","Sehingga menghasilkan performa yang buruk dalam pelatihan data.\n","\n","• Dikarenakan output tersebut menghasilkan training loss dan validation loss yang terus\n","menurun, maka perlu menaikkan epoch dua kali lipatnya untuk percobaan selanjutnya.\n","Akan tetapi pada F1-Score sudah mendapatkan angka yang optimal yaitu 0.85, maka model ini yang akan kita gunakan untuk testing"],"metadata":{"id":"inx56pRBOvyy"}},{"cell_type":"markdown","source":["**9. LOADING FINETUNED BERT MODEL AND EVALUATE ITS PERFORMANCE**"],"metadata":{"id":"tojJZK8nLER3"}},{"cell_type":"code","source":["model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                      num_labels=len(label_dict),\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)\n","\n","model.to(device)"],"metadata":{"id":"pwFuLJoWHGJ5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**LOADING BEST BERT MODEL**"],"metadata":{"id":"imlqueNaLOyu"}},{"cell_type":"code","source":["model.load_state_dict(torch.load('/content/finetuned_BERT_epoch_3.model', map_location=torch.device('cpu')))"],"metadata":{"id":"mdlcI621HMfj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, predictions, true_vals = evaluate(dataloader_validation)"],"metadata":{"id":"VYWjS2syHT-O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracy_per_class(predictions, true_vals)"],"metadata":{"id":"kS4q7KSFHXmZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["dari model epoch 3, terdapat hasil bahwa model ini memiliki akurais yang tinggi dalam memprediksi kelas kelasnya, seperti pada class technology, dapat dilihat bahwa model ini bekerja dengan nilai akurasi 27 dari 37, sehingga dapat dikatakan model ini memiliki akurasi yang tinggi"],"metadata":{"id":"MBVk7u4xPaC8"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}